{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlPnTMNlvgKW",
        "outputId": "af5255a0-b60e-447a-e75b-dc11041268cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Optimal Policy:\n",
            "State: 0, Action: right\n",
            "State: 1, Action: right\n",
            "State: 2, Action: right\n",
            "State: 3, Action: right\n",
            "State: 4, Action: right\n",
            "State: 5, Action: right\n",
            "State: 6, Action: right\n",
            "State: 7, Action: right\n",
            "State: 8, Action: right\n",
            "State: 9, Action: right\n",
            "State: 10, Action: right\n",
            "State: 11, Action: right\n",
            "State: 12, Action: right\n",
            "State: 13, Action: right\n",
            "State: 14, Action: left\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class CustomEnv:\n",
        "    def __init__(self):\n",
        "        self.num_states = 15\n",
        "        self.actions = ['left', 'right']\n",
        "        self.max_steps = 50\n",
        "        self.current_state = 0\n",
        "        self.reward = 0\n",
        "        self.total_reward = 1\n",
        "        self.steps = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_state = 0\n",
        "        self.reward = 0\n",
        "        self.total_reward = 1\n",
        "        self.steps = 0\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        if action == 'right':\n",
        "            self.current_state += 1\n",
        "            self.reward = 1 if self.current_state == self.num_states - 1 else 0\n",
        "        else:\n",
        "            self.current_state -= 1 if self.current_state > 0 else 0\n",
        "            self.reward = 0\n",
        "\n",
        "        self.total_reward += self.reward\n",
        "\n",
        "        done = self.current_state == self.num_states - 1 or self.steps >= self.max_steps\n",
        "        return self.current_state, self.reward, done, {}\n",
        "\n",
        "# Monte Carlo Control\n",
        "def monte_carlo_control(env, episodes=1000, gamma=1.0):\n",
        "    returns_sum = {}\n",
        "    returns_count = {}\n",
        "    Q = {}\n",
        "    policy = {}\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        states_actions_returns = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # Generate an episode\n",
        "        while not done:\n",
        "            action = random.choice(env.actions)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            states_actions_returns.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        # Update Q values\n",
        "        G = 0\n",
        "        for i, (state, action, reward) in enumerate(reversed(states_actions_returns)):\n",
        "            G = gamma * G + reward\n",
        "            if (state, action) not in [(x[0], x[1]) for x in states_actions_returns[::-1][i+1:]]:\n",
        "                if (state, action) in returns_sum:\n",
        "                    returns_sum[(state, action)] += G\n",
        "                    returns_count[(state, action)] += 1\n",
        "                else:\n",
        "                    returns_sum[(state, action)] = G\n",
        "                    returns_count[(state, action)] = 1\n",
        "                Q[(state, action)] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
        "\n",
        "        # Update policy based on Q-values\n",
        "        for s in range(env.num_states):\n",
        "            best_actions = [act for act in env.actions if (s, act) in Q.keys() and Q[(s, act)] == max([Q[(s, a)] for a in env.actions if (s, a) in Q.keys()])]\n",
        "            policy[s] = random.choice(best_actions) if best_actions else random.choice(env.actions)\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "# Create an instance of the environment\n",
        "env = CustomEnv()\n",
        "\n",
        "# Monte Carlo Control for learning optimal policy\n",
        "Q, optimal_policy = monte_carlo_control(env)\n",
        "\n",
        "# Displaying the learned optimal policy\n",
        "print(\"Learned Optimal Policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State: {state}, Action: {action}\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}