{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Neural network for the agent\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Agent class\n",
        "class Agent:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.policy_network = PolicyNetwork(input_size, output_size)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.01)\n",
        "        self.output_size = output_size  # Store output size\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float()\n",
        "        probabilities = self.policy_network(state)\n",
        "        probabilities = probabilities.detach().numpy()  # Convert to numpy array\n",
        "        action = np.random.choice(np.arange(self.output_size), p=probabilities)\n",
        "        return action\n",
        "\n",
        "# Training loop\n",
        "agent = Agent(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
        "num_episodes = 300\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        agent.optimizer.zero_grad()\n",
        "        state_tensor = torch.from_numpy(state).float()\n",
        "        action_tensor = torch.tensor(action)\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "\n",
        "        log_prob = torch.log(agent.policy_network(state_tensor)[action_tensor])\n",
        "        loss = -log_prob * reward_tensor\n",
        "        loss.backward()\n",
        "        agent.optimizer.step()\n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {episode_reward}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiCwLctyfMBu",
        "outputId": "95598593-b9cc-4122-c4ba-1953be453a56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 46.0\n",
            "Episode 10, Total Reward: 10.0\n",
            "Episode 20, Total Reward: 10.0\n",
            "Episode 30, Total Reward: 8.0\n",
            "Episode 40, Total Reward: 8.0\n",
            "Episode 50, Total Reward: 9.0\n",
            "Episode 60, Total Reward: 9.0\n",
            "Episode 70, Total Reward: 10.0\n",
            "Episode 80, Total Reward: 10.0\n",
            "Episode 90, Total Reward: 10.0\n",
            "Episode 100, Total Reward: 9.0\n",
            "Episode 110, Total Reward: 9.0\n",
            "Episode 120, Total Reward: 10.0\n",
            "Episode 130, Total Reward: 10.0\n",
            "Episode 140, Total Reward: 10.0\n",
            "Episode 150, Total Reward: 8.0\n",
            "Episode 160, Total Reward: 9.0\n",
            "Episode 170, Total Reward: 10.0\n",
            "Episode 180, Total Reward: 9.0\n",
            "Episode 190, Total Reward: 11.0\n",
            "Episode 200, Total Reward: 10.0\n",
            "Episode 210, Total Reward: 9.0\n",
            "Episode 220, Total Reward: 10.0\n",
            "Episode 230, Total Reward: 9.0\n",
            "Episode 240, Total Reward: 9.0\n",
            "Episode 250, Total Reward: 10.0\n",
            "Episode 260, Total Reward: 10.0\n",
            "Episode 270, Total Reward: 9.0\n",
            "Episode 280, Total Reward: 10.0\n",
            "Episode 290, Total Reward: 9.0\n"
          ]
        }
      ]
    }
  ]
}