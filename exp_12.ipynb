{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhju28RUfzrl",
        "outputId": "3a681ae7-87e8-42a0-d681-5f872e44252c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Values:\n",
            "[21.80433863 25.06520819  0.        ]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP parameters\n",
        "n_states = 3  # Number of states\n",
        "n_actions = 2  # Number of actions\n",
        "\n",
        "# Define the MDP transition probabilities and rewards\n",
        "# Transitions: state -> action -> next state\n",
        "P = np.zeros((n_states, n_actions, n_states))\n",
        "P[0, 0, 0] = 0.7\n",
        "P[0, 0, 1] = 0.3\n",
        "P[0, 1, 1] = 0.5\n",
        "P[0, 1, 2] = 0.5\n",
        "P[1, 0, 0] = 0.4\n",
        "P[1, 0, 1] = 0.6\n",
        "P[1, 1, 0] = 0.1\n",
        "P[1, 1, 1] = 0.9\n",
        "P[2, 0, 2] = 1.0\n",
        "P[2, 1, 2] = 1.0\n",
        "\n",
        "# Rewards: state -> action -> next state\n",
        "R = np.zeros((n_states, n_actions, n_states))\n",
        "R[0, 0, 0] = 1.0\n",
        "R[0, 0, 1] = 2.0\n",
        "R[0, 1, 1] = 3.0\n",
        "R[0, 1, 2] = 4.0\n",
        "R[1, 0, 0] = 0.0\n",
        "R[1, 0, 1] = 2.0\n",
        "R[1, 1, 0] = 1.0\n",
        "R[1, 1, 1] = 3.0\n",
        "R[2, 0, 2] = 0.0\n",
        "R[2, 1, 2] = 0.0\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration(P, R, gamma, epsilon=1e-6):\n",
        "    n_states, n_actions, _ = P.shape\n",
        "    V = np.zeros(n_states)\n",
        "\n",
        "    while True:\n",
        "        V_new = np.zeros(n_states)\n",
        "\n",
        "        for s in range(n_states):\n",
        "            Q_s = np.zeros(n_actions)\n",
        "\n",
        "            for a in range(n_actions):\n",
        "                for s_prime in range(n_states):\n",
        "                    Q_s[a] += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
        "\n",
        "            V_new[s] = np.max(Q_s)\n",
        "\n",
        "        if np.max(np.abs(V - V_new)) < epsilon:\n",
        "            break\n",
        "\n",
        "        V = V_new.copy()\n",
        "\n",
        "    return V\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.9  # Discount factor\n",
        "\n",
        "# Perform Value Iteration\n",
        "optimal_values = value_iteration(P, R, gamma)\n",
        "print(\"Optimal Values:\")\n",
        "print(optimal_values)\n"
      ]
    }
  ]
}